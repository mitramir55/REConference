{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A complete guide on cleaning textual data\n",
    "\n",
    "In this tutorial, we’ll take a look at the general and most important steps of text cleaning before giving it to a machine learning or deep learning model.\n",
    "\n",
    "Building deep learning or machine learning models takes weeks, days, or at least a few hours. To  improve the results, we utilize various packages, add layers, and apply different techniques. But what if looking at the data itself and modifying it a little bit, could help us with both performance and the training time of our model?\n",
    "\n",
    "\n",
    "If we look at textual data that hasn't been altered before, we'll see that people tend to use language as they please and without any special consideration of grammar or structure.  As a result, their words and sentences would be just a series of characters that can’t be properly distinguished and interpreted by our natural language processing algorithms and models. This makes our work a bit harder, meaning that we have to better clarify and prepare the data for our final algorithm.[1]\n",
    "\n",
    "\n",
    "But fear not! As Tomas Mikolov, one of the authors of Word2vec famous text processing algorithms says, building a deep learning model with the ability to learn the semantic relationships between words requires as little cleaning as possible. Because these models are very apt at understanding which parts of the text they need to focus on (pay attention to), to achieve their objective. However, still, even a little cleaning will play a big role, as it reduces memory usage by shrinking the vocabulary size and helps you identify more words by deleting unnecessary characters around them.\n",
    "\n",
    "Here is a few steps we can take to improve and clean our text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For cleaning the text\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Substituting emojis and emoticons\n",
    "In cleaning, you might prefer to remove all the punctuations at first and therefore the all the emoticons that are made from them, like :), :( and :|. But by doing this you’re actually removing parts of the meaning. The better way of handling punctuations is to first try to substitute these parts and then delete the remaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \n",
    "    # Happy \n",
    "    text = re.sub(\":D\", 'grin',text)\n",
    "    text = re.sub(\" (x|X)D\", 'laugh',text)\n",
    "    text = re.sub(\":\\)+\", 'happy',text)\n",
    "\n",
    "    # Sad \n",
    "    \n",
    "    text = re.sub(\":\\(+\", 'sad',text)\n",
    "    text = re.sub(\"-_+-\", 'annoyed',text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is so creepy! grin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example sentence\n",
    "\n",
    "text = 'This is so creepy! :D'\n",
    "remove_emojis(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "We all know how frequently words like ‘is’, ‘are’, ‘am’, ‘he’, ‘she’, are used. These words are called stopwords, and they’re so commonly used that appear in all sorts and types of sentences. They don’t have any specific information to add to a sentence that may change the meaning completely, so we simply ignore them while performing tasks like text classification. Google often ignores them [2] when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "There are different libraries like nltk and spacy with different sets and number of stop words, so depending on how much and what stopwords you want to remove, you can choose one.( NLTK has around 180, but Spacy has around 360 stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurate !\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "text = 'This is not accurate!'\n",
    "\n",
    "text = [word for word in word_tokenize(text) if not word.lower() in stop_words]\n",
    "text = ' '.join(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "Punctuations can have a big impact on the emotion expressed in the writing of the text. But sometimes, we don't care about the feelings in a database and want to create more clarity by removing these repetitive pieces of text that don't impart any further knowledge. \n",
    "\n",
    " Using `maketrans` is one of the best way of replacing punctuation with a blank space. In the following cell, you see that the third argument lists what should be removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing\n",
    "Usually, lower casing can hugely reduce the size of the vocabulary. It will substitute all the capitalized letters with their small form like, “Another”, “There”, will become “another”, “There”. But pay close attention that at the same time it robs some words like “Bush”, “Bill”, “Apple” form their accurate representation and meaning by turning them into “bush”, “bill”, “apple”. You can simply lowercase your words with .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple represents itself in new york.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "text = 'Apple represents itself in New York.'\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization or stemming?\n",
    "\n",
    "Purposes of lemmatization and stemming is the similar. They both want to relate different forms of verbs, nouns, in general words, to their base form, but they do this in different ways.\n",
    "\n",
    "\n",
    " Stemming is the process of chopping off the end of words in the hope of getting a simple and correct shape of the words. But lemmatization is the process of doing this properly with the use of a dictionary. So if we give “studies” to a stemmer, it will return “studi”, but if we give it to a lemmatizer, it will output “study”. Both of these functions tend to reduce your vocabulary size and variety in your text. So be careful about the tradeoff between the performance of model and the information that remains.\n",
    "\n",
    " ### Lemmatization with nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7488/2443869121.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "words = [\"programs\", \"programer\", \"programing\", \"programers\", \"studies\", \"cries\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional steps\n",
    "\n",
    "And for the last step, if you’re using a deep learning model with a dictionary, you go through your dataset and check which words were not recognized by your algorithm and then try to find out ways that can reduce those words. You may even consider manually correcting some words like “Goooaaaal” or “Snaaap”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of some text we usually have to scrape in scraped data\n",
    "rep_character_entity_refrences = {\"&gt;\": \">\", \"&lt;\":\"<\", \"&amp;\": \"&\"}\n",
    "\n",
    "# removing links: search for http and continue removing until you hit a space\n",
    "text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "\n",
    "# When you only want to keep words and certain characters\n",
    "text = re.sub(r'[^ \\w\\.\\-\\(\\)\\,]', ' ', text)\n",
    "\n",
    "# removes all single letters (typos) surrounded by space except the letters I and a\n",
    "text = re.sub(r' +(?![ia])[a-z] +', ' ', text)\n",
    "\n",
    "# removes all hashtags and the text right after them #peace\n",
    "text = re.sub(r'[@#]\\w*\\_*' , '', text)\n",
    "\n",
    "# substitute extra space with only one space\n",
    "text = re.sub(r' \\s+', ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "In this era of history, we see computers and machines rise in every aspect of our lives, doing our chores, connecting us, and etc. In return we have to help them understand our language better and make the interaction easier for both us humans and machines. Cleaning is just one of the ways that’ll help us make faster and more accurate models, and because it’s modifying the main text, we have to be careful and construct a good procedure to remove as little meaning and parts as possible.\n",
    "\n",
    "Thanks for reading this article!\n",
    "\n",
    "### References\n",
    "\n",
    "[1] https://www.kaggle.com/code/mitramir5/simple-bert-with-video\n",
    "[2] https://bloggingx.com/stop-words/#:~:text=Search%20engines%2C%20in%20both%20search,are%20ignored%20or%20filtered%20out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adfccd5c958d70cd78f22c942a5f90dd259f52bf203482d766a0ca6ec5e2e798"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
