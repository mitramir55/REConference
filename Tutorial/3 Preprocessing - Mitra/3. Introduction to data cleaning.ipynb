{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ A complete guide on cleaning textual data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, weâ€™ll take a look at the general and most important steps of text cleaning before giving it to a machine learning or deep learning model.\n",
    "\n",
    "Building deep learning or machine learning models takes weeks, days, or at least a few hours. To  improve the results, we utilize various packages, add layers, and apply different techniques. But what if looking at the data itself and modifying it a little bit, could help us with both performance and the training time of our model?\n",
    "\n",
    "\n",
    "If we look at textual data that hasn't been altered before, we'll see that people tend to use language as they please and without any special consideration of grammar or structure.  As a result, their words and sentences would be just a series of characters that canâ€™t be properly distinguished and interpreted by our natural language processing algorithms and models. This makes our work a bit harder, meaning that we have to better clarify and prepare the data for our final algorithm.[1]\n",
    "\n",
    "\n",
    "But fear not! As Tomas Mikolov, one of the authors of Word2vec famous text processing algorithms says, building a deep learning model with the ability to learn the semantic relationships between words requires as little cleaning as possible. Because these models are capable of understanding which parts of the text to focus on (pay attention to), to achieve their objective. However, still, even a little cleaning will play a big role, as it reduces memory usage by shrinking the vocabulary size and helps you identify more words by deleting unnecessary characters around them.\n",
    "\n",
    "Here are a few steps we can take to improve and clean our text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Substituting Emojis\n",
    "* Removing Stopwords\n",
    "* Removing Punctuations\n",
    "* Lower casing\n",
    "* Lemmatization\n",
    "* Stemming\n",
    "* Tokenization\n",
    "* Additional Resources\n",
    "* Conclusion\n",
    "* References\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*B09y6eYoPTbHspcEoQUTNQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For cleaning the text\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Substituting emojis and emoticons\n",
    "In cleaning, you might prefer to remove all the punctuations at first and therefore the all the emoticons that are made from them, like :), :( and :|. But by doing this youâ€™re actually removing parts of the meaning. The better way of handling punctuations is to first try to substitute these parts and then delete the remaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \n",
    "    # Happy \n",
    "    text = re.sub(\":D\", 'grin',text)\n",
    "    text = re.sub(\" (x|X)D\", 'laugh',text)\n",
    "    text = re.sub(\":\\)+\", 'happy',text)\n",
    "\n",
    "    # Sad \n",
    "    \n",
    "    text = re.sub(\":\\(+\", 'sad',text)\n",
    "    text = re.sub(\"-_+-\", 'annoyed',text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is so creepy! grin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example sentence\n",
    "\n",
    "text = 'This is so creepy! :D'\n",
    "remove_emojis(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "We all know how frequently words like â€˜isâ€™, â€˜areâ€™, â€˜amâ€™, â€˜heâ€™, â€˜sheâ€™, are used. These words are called stopwords, and theyâ€™re so commonly used that appear in all sorts and types of sentences. They donâ€™t have any specific information to add to a sentence that may change the meaning completely, so we simply ignore them while performing tasks like text classification. Google often ignores them [2] when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "There are different libraries like nltk and spacy with different sets and number of stop words, so depending on how much and what stopwords you want to remove, you can choose one.( NLTK has around 180, but Spacy has around 360 stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurate !\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "text = 'This is not accurate!'\n",
    "\n",
    "text = [word for word in word_tokenize(text) if not word.lower() in stop_words]\n",
    "text = ' '.join(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "Punctuations can have a big impact on the emotion expressed in the writing of the text. But sometimes, we don't care about the feelings in a database and want to create more clarity by removing these repetitive pieces of text that don't impart any further knowledge. \n",
    "\n",
    " In the following cell, we use regex to find any of the punctuations in the brackets and substitute them with blank space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    return re.sub(\"[()!><.,`?':\\-\\[\\]_@]\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is crazy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "remove_punct('this is... crazy!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing\n",
    "Usually, lower casing can hugely reduce the size of the vocabulary. It will substitute all the capitalized letters with their small form like, â€œAnotherâ€, â€œThereâ€, will become â€œanotherâ€, â€œThereâ€. But pay close attention that at the same time it robs some words like â€œBushâ€, â€œBillâ€, â€œAppleâ€ form their accurate representation and meaning by turning them into â€œbushâ€, â€œbillâ€, â€œappleâ€. You can simply lowercase your words with .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple represents itself in new york.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "text = 'Apple represents itself in New York.'\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization or stemming?\n",
    "\n",
    "Purposes of lemmatization and stemming is the similar. They both want to relate different forms of verbs, nouns, in general words, to their base form, but they do this in different ways.\n",
    "\n",
    "\n",
    " Stemming is the process of chopping off the end of words in the hope of getting a simple and correct shape of the words. But lemmatization is the process of doing this properly with the use of a dictionary. So if we give â€œstudiesâ€ to a stemmer, it will return â€œstudiâ€, but if we give it to a lemmatizer, it will output â€œstudyâ€. Both of these functions tend to reduce your vocabulary size and variety in your text. So be careful about the tradeoff between the performance of model and the information that remains.\n",
    "\n",
    " ### Lemmatization with NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitra\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n",
      "studies  :  studi\n",
      "cries  :  cri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "words = [\"programs\", \"programer\", \"programing\", \"programers\", \"studies\", \"cries\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Before cleaning, we can tokenize the data and create a list of all the words in our records. This way, instead of looking at the whole text, we look at individual words. The benefit of this approach is a higher accuracy while choosing which words we want to remove.\n",
    "\n",
    "Look at the following example for understanding the concept better:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " '!',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'Mary',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'reporting',\n",
       " 'an',\n",
       " 'issue']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello! I'm Mary, and I'm reporting an issue\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to clean text and remove stopwords, for instance, we remove both \"I\" and \"I'm\". While if we were to not tokenize the text, we would have ended up with \"Im\" after the removal of punctuations, and not identify this piece of text as a stop word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional steps\n",
    "\n",
    "And for the last step, you go through your dataset and check which words were not recognized by your algorithm and then try to find out ways that can reduce those words. You may even consider manually correcting some words like â€œGoooaaaalâ€ or â€œSnaaapâ€.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_cleaning(text):\n",
    "\n",
    "    # list of some text we usually have to scrape in scraped data\n",
    "    character_entity_references_dict = {\"&gt;\": \">\", \"&lt;\":\"<\", \"&amp;\": \"&\"}\n",
    "    for pattern, replacement in character_entity_references_dict.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    # removing links: search for http and continue removing until you hit a space\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
    "\n",
    "    # When you only want to keep words and certain characters\n",
    "    text = re.sub(r'[^ \\w\\.\\-\\(\\)\\,]', ' ', text)\n",
    "\n",
    "    # removes all single letters (typos) surrounded by space except the letters I and a\n",
    "    text = re.sub(r' +(?![ia])[a-z] +', ' ', text)\n",
    "\n",
    "    # removes all hashtags and the text right after them #peace\n",
    "    text = re.sub(r'[@#]\\w*\\_*' , '', text)\n",
    "\n",
    "    # substitute extra space with only one space\n",
    "    text = re.sub(r' \\s+', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can look at my website to learn more about this topic cool '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You can    look at my website https://regexr.com/ to learn more about     this topic! #cool !  c    '\n",
    "\n",
    "additional_cleaning(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "In this era of history, we see computers and machines help us in every aspect of our lives! In return we have to help them understand our language better and make the interaction easier for both us humans and machines. \n",
    "\n",
    "Cleaning is just one of the ways that bring about faster and more accurate models. But because itâ€™s modifying the main text, we have to be careful to construct functions that remove as little as possible from the text and its essential parts.\n",
    "\n",
    "Thanks for reading this article!\n",
    "\n",
    "### References\n",
    "\n",
    "[1] https://www.kaggle.com/code/mitramir5/simple-bert-with-video\n",
    "\n",
    "[2] https://bloggingx.com/stop-words/#:~:text=Search%20engines%2C%20in%20both%20search,are%20ignored%20or%20filtered%20out.\n",
    "\n",
    "Image credit: https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/\n",
    "\n",
    "https://www.kaggle.com/code/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n",
    "\n",
    "https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n",
    "\n",
    "https://regexr.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adfccd5c958d70cd78f22c942a5f90dd259f52bf203482d766a0ca6ec5e2e798"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
