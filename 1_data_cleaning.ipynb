{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitramir55/REConference/blob/main/1_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_xWHuU5gtv"
      },
      "source": [
        "## ðŸ§¹ A complete guide on cleaning textual data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyeZF_Tc5gt0"
      },
      "source": [
        "In this tutorial, weâ€™ll take a look at the general and most important steps of text cleaning before giving it to a machine learning or deep learning model.\n",
        "\n",
        "Building deep learning or machine learning models takes weeks, days, or at least a few hours. To  improve the results, we utilize various packages, add layers, and apply different techniques. But what if looking at the data itself and modifying it a little bit, could help us with both performance and the training time of our model!\n",
        "\n",
        "\n",
        "But keep in mind that building a deep learning model with the ability to learn the semantic relationships between words requires as little cleaning as possible. Because these models are capable of understanding which parts of the text to focus on (pay attention to), to achieve their objective. However, still, even a little cleaning will play a big role, as it reduces memory usage by shrinking the vocabulary size and helps you identify more words by deleting unnecessary characters around them.\n",
        "\n",
        "Here are a few steps we can take to improve and clean our text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXsibsre5gt1"
      },
      "source": [
        "* Substituting Emojis\n",
        "* Removing Stopwords\n",
        "* Removing Punctuations\n",
        "* Lower casing\n",
        "* Lemmatization\n",
        "* Stemming\n",
        "* Tokenization\n",
        "* Additional Resources\n",
        "* Conclusion\n",
        "* References\n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*B09y6eYoPTbHspcEoQUTNQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_MIwJfC5gt2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK packages for text cleaning\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhRE5TgUf-3d",
        "outputId": "87a99b9e-ae85-4046-9de8-fa83c320c668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we'll be using regular expressions alot. A regular expression is a string of letters that defines a text search pattern. So if you needed any help or had any question about any expression, simply go to https://regexr.com/ and explore different patterns and see what each one matches to. "
      ],
      "metadata": {
        "id": "B79JdcLtN3RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re"
      ],
      "metadata": {
        "id": "V2oPcdEogTyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF7zrP4f5gt4"
      },
      "source": [
        "\n",
        "### Substituting emojis and emoticons\n",
        "In cleaning, you might prefer to remove all the punctuations at first and therefore the all the emoticons that are made from them, like :), :( and :|. But by doing this youâ€™re actually removing parts of the meaning. The better way of handling punctuations is to first try to substitute these parts and then delete the remaining.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5s4HS-K5gt4"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    \n",
        "    # Happy \n",
        "    text = re.sub(\":D\", 'grin',text)\n",
        "    text = re.sub(\" (x|X)D\", 'laugh',text)\n",
        "    text = re.sub(\":\\)+\", 'happy',text)\n",
        "\n",
        "    # Sad \n",
        "    text = re.sub(\":\\(+\", 'sad',text)\n",
        "    text = re.sub(\"-_+-\", 'annoyed',text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_E0cCb6e5gt5",
        "outputId": "c984fefc-1065-4e1f-8cde-affd7c0f0c98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is so creepy! grin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# example sentence\n",
        "\n",
        "text = 'This is so creepy! :D'\n",
        "remove_emojis(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNtAt5G45guA"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Before cleaning, we can tokenize the data and create a list of all the words in our records. This way, instead of looking at the whole text, we look at individual words. The benefit of this approach is a higher accuracy while choosing which words we want to remove.\n",
        "\n",
        "Look at the following example for understanding the concept better:\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download punkt which helps in tokenizing text\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "GKwOWUX_hRVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c331960-9c79-4558-9ba0-d1ff38775ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4BsrfSs5guB",
        "outputId": "74a79615-604f-4b3e-e02e-aa2a865c733c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'Mary',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'reporting',\n",
              " 'an',\n",
              " 'issue']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text = \"Hello! I'm Mary, and I'm reporting an issue\"\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSLrRPXC5guB"
      },
      "source": [
        "Now, to clean text and remove stopwords, for instance, we remove both \"I\" and \"I'm\". While if we were to not tokenize the text, we would have ended up with \"Im\" after the removal of punctuations, and not identify this piece of text as a stop word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnOOX3tn5gt6"
      },
      "source": [
        "### Removing Stopwords\n",
        "We all know how frequently words like â€˜isâ€™, â€˜areâ€™, â€˜amâ€™, â€˜heâ€™, â€˜sheâ€™, are used. These words are called stopwords, and theyâ€™re so commonly used that appear in all sorts and types of sentences. They donâ€™t add any specific information to a sentence that may change the meaning completely. Google often ignores them [2] when indexing entries for searching and when retrieving them as the result of a search query.\n",
        "\n",
        "\n",
        "\n",
        "There are different libraries like[ nltk ](https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default)and [spacy](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py) with different sets and number of stop words, so depending on how much and what stopwords you want to remove, you can choose one (NLTK has around 180, but Spacy has around 360 stop words).\n",
        "\n",
        "Read more: [link](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kph_Kt95gt7",
        "outputId": "3c684727-6c06-4399-924a-da290eb4bb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# download nktj stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# you can either use default stopwords or customize it\n",
        "stop_words = stopwords.words('english')\n",
        "#stop_words = [\"the\", \"and\", \"not\"]\n",
        "\n",
        "print(len(stop_words))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bYlePw45gt8",
        "outputId": "380ff41b-03c0-40e7-ad0a-a25688358c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accurate !\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "text = 'This is not accurate!'\n",
        "\n",
        "text = [word for word in word_tokenize(text) if not word.lower() in stop_words]\n",
        "text = ' '.join(text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Qr48Af5gt8"
      },
      "source": [
        "### Removing Punctuations\n",
        "Punctuations can have a big impact on the emotion expressed in the writing of the text. But sometimes, we don't care about the feelings in a database and want to create more clarity by removing these repetitive pieces of text that don't impart any further knowledge. \n",
        "\n",
        " In the following cell, we use regex to find any of the punctuations in the brackets and substitute them with blank space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE2qaFDy5gt9"
      },
      "outputs": [],
      "source": [
        "def remove_punct(text):\n",
        "    return re.sub(\"[()!><.,`?':\\-\\[\\]_@]\", '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "aT5xKOBv5gt9",
        "outputId": "b6998cba-f2ab-4531-ac88-668a3fe1fdff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is crazy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# example\n",
        "remove_punct('this is... crazy!!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmsGXK895gt-"
      },
      "source": [
        "### Lower casing\n",
        "Usually, lower casing can hugely reduce the size of the vocabulary. It will substitute all the capitalized letters with their small form like, â€œAnotherâ€, â€œThereâ€, will become â€œanotherâ€, â€œThereâ€. But pay close attention that at the same time it robs some words like â€œBushâ€, â€œBillâ€, â€œAppleâ€ form their accurate representation and meaning by turning them into â€œbushâ€, â€œbillâ€, â€œappleâ€. You can simply lowercase your words with .lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TIPkmMOl5gt-",
        "outputId": "13da372e-584b-484e-b3b2-f852dae2f50f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple represents itself in new york.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# example\n",
        "text = 'Apple represents itself in New York.'\n",
        "text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3NN1I9R5gt_"
      },
      "source": [
        "### Lemmatization or stemming?ðŸ¤”\n",
        "\n",
        "Purposes of lemmatization and stemming is the similar. They both want to relate different forms of verbs, nouns, in general words, to their base form, but they do this in different ways.\n",
        "\n",
        "\n",
        " Stemming is the process of chopping off the end of words in the hope of getting a simple and correct shape of the words. But lemmatization is the process of doing this properly with the use of a dictionary. So if we give â€œstudiesâ€ to a stemmer, it will return â€œstudiâ€, but if we give it to a lemmatizer, it will output â€œstudyâ€. Both of these functions tend to reduce your vocabulary size and variety in your text. So be careful about the tradeoff between the performance of model and the information that remains.\n",
        "\n",
        " ### Lemmatization with NLTK:\n",
        "\n",
        " NLTK lemmatizer uses WordNet. WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets) which are interlinked by means of conceptual-semantic and lexical relations. Read more about this database [here](https://wordnet.princeton.edu/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNmVJvai5gt_",
        "outputId": "05c7e44a-6260-43fa-9bd4-8503f5f518b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# download wordnet for lemmatization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91aFlm215gt_",
        "outputId": "fceec751-5cd6-4a43-ae7c-ece221a799e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
        "  \n",
        "# a denotes adjective in \"pos\" \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbc5ECd5guA"
      },
      "source": [
        "### Stemming with NLTK\n",
        "NLTK uses [PorterStemmer](https://www.nltk.org/_modules/nltk/stem/porter.html) to stem words. Porter stemming algorithm is capable of removing endings of words in text normalization. Read more about this algorithm [here](https://tartarus.org/martin/PorterStemmer/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRpCn57O5guA",
        "outputId": "954826ea-7c98-4c57-85d3-d60b516f3d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n",
            "studies  :  studi\n",
            "cries  :  cri\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "ps = PorterStemmer()\n",
        "words = [\"programs\", \"programer\", \"programing\", \"programers\", \"studies\", \"cries\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyV8ZbOg5guC"
      },
      "source": [
        "### Some additional steps\n",
        "\n",
        "And for the last step, you go through your dataset and check which words were not recognized by your algorithm and then try to find out ways that can reduce those words. You may even consider manually correcting some words like â€œGoooaaaalâ€ or â€œSnaaapâ€.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOQjtUrQ5guC"
      },
      "outputs": [],
      "source": [
        "def additional_cleaning(text):\n",
        "\n",
        "    # list of some text we usually have to scrape in scraped data\n",
        "    character_entity_references_dict = {\"&gt;\": \">\", \"&lt;\":\"<\", \"&amp;\": \"&\"}\n",
        "    for pattern, replacement in character_entity_references_dict.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # removing links: search for http and continue removing until you hit a space\n",
        "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text)\n",
        "\n",
        "    # When you only want to keep words and certain characters\n",
        "    text = re.sub(r'[^ \\w\\.\\-\\(\\)\\,]', ' ', text)\n",
        "\n",
        "    # removes all single letters (typos) surrounded by space except the letters I and a\n",
        "    text = re.sub(r' +(?![ia])[a-z] +', ' ', text)\n",
        "\n",
        "    # removes all hashtags and the text right after them #peace\n",
        "    text = re.sub(r'[@#]\\w*\\_*' , '', text)\n",
        "\n",
        "    # substitute extra space with only one space\n",
        "    text = re.sub(r' \\s+', ' ', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "KQcMWQPi5guC",
        "outputId": "6625e324-d4aa-4f6d-eed4-e3fae9c2359b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You can look at my website to learn more about this topic cool '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "text = 'You can    look at my website https://regexr.com/ to learn more about     this topic! #cool !  c    '\n",
        "\n",
        "additional_cleaning(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Further Parsing of words with SpaCy\n",
        "\n",
        "SpaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. Here, we'll use this library to parse sentences further.\n",
        "\n",
        "Description of various SpaCy models: [link](https://spacy.io/models/en)"
      ],
      "metadata": {
        "id": "ETzDXIa5oP0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# download the smallest language model available on SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "yX_kMg4mqcKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you needed to analyze each token of a sentence, i.g., print out the `DEP` (dependency), `POS` (coarse-grained part of speech tags), `TAG` (fine-grained part of speech tags), `LEMMA` (canonical form) of a word, you can use the following tags to see the features extracted by SpaCy language model.\n",
        "\n",
        "To know more about each of these topics go to the links listed:\n",
        "* Dependency parsing: [Stanford typed dependencies manual](https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf)\n",
        "\n",
        "* POS tagging: [Spacy POS tagger](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
        "\n",
        "* Labels definition: [Spacy Glossary](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py) and [CoNLL-U Format](https://spacy.io/models/en)"
      ],
      "metadata": {
        "id": "EDeT5FvIq0kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "words_features = []\n",
        "\n",
        "for token in doc:\n",
        "    word_dict = {}\n",
        "    word_dict['word'] = token.text\n",
        "    word_dict['lemma_'] = token.lemma_\n",
        "    word_dict['pos_'] = token.pos_\n",
        "    word_dict['tag_'] = token.tag_\n",
        "    word_dict['dep_'] = token.dep_\n",
        "    word_dict['shape_'] = token.shape_\n",
        "    word_dict['is_alpha'] = token.is_alpha\n",
        "    word_dict['is_stop'] = token.is_stop\n",
        "\n",
        "    words_features.append(word_dict)\n",
        "\n",
        "pd.DataFrame(words_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "iYQGyF_3oPip",
        "outputId": "17ecbd10-3ceb-4fc1-e5cf-70d6821b5059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       word   lemma_   pos_ tag_      dep_ shape_  is_alpha  is_stop\n",
              "0     Apple    Apple  PROPN  NNP     nsubj  Xxxxx      True    False\n",
              "1        is       be    AUX  VBZ       aux     xx      True     True\n",
              "2   looking     look   VERB  VBG      ROOT   xxxx      True    False\n",
              "3        at       at    ADP   IN      prep     xx      True     True\n",
              "4    buying      buy   VERB  VBG     pcomp   xxxx      True    False\n",
              "5      U.K.     U.K.  PROPN  NNP  compound   X.X.     False    False\n",
              "6   startup  startup   NOUN   NN      dobj   xxxx      True    False\n",
              "7       for      for    ADP   IN      prep    xxx      True     True\n",
              "8         $        $    SYM    $  quantmod      $     False    False\n",
              "9         1        1    NUM   CD  compound      d     False    False\n",
              "10  billion  billion    NUM   CD      pobj   xxxx      True    False"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15b393a5-e641-4e12-9ed6-c8608877ba3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>lemma_</th>\n",
              "      <th>pos_</th>\n",
              "      <th>tag_</th>\n",
              "      <th>dep_</th>\n",
              "      <th>shape_</th>\n",
              "      <th>is_alpha</th>\n",
              "      <th>is_stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Apple</td>\n",
              "      <td>Apple</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>Xxxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>be</td>\n",
              "      <td>AUX</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>aux</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>look</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>buy</td>\n",
              "      <td>VERB</td>\n",
              "      <td>VBG</td>\n",
              "      <td>pcomp</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>compound</td>\n",
              "      <td>X.X.</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>startup</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>NN</td>\n",
              "      <td>dobj</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>for</td>\n",
              "      <td>ADP</td>\n",
              "      <td>IN</td>\n",
              "      <td>prep</td>\n",
              "      <td>xxx</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>$</td>\n",
              "      <td>SYM</td>\n",
              "      <td>$</td>\n",
              "      <td>quantmod</td>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>compound</td>\n",
              "      <td>d</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>billion</td>\n",
              "      <td>NUM</td>\n",
              "      <td>CD</td>\n",
              "      <td>pobj</td>\n",
              "      <td>xxxx</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15b393a5-e641-4e12-9ed6-c8608877ba3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15b393a5-e641-4e12-9ed6-c8608877ba3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15b393a5-e641-4e12-9ed6-c8608877ba3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "xnblQyjkpWBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Bears are dreamt of in your fantasies.\"\n",
        "doc = nlp(txt)\n",
        "\n",
        "svg = displacy.render(doc, jupyter=True, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "Q8zVXk0kxIS0",
        "outputId": "aa4ef65a-58c6-4292-be94-883c8bb4b471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9c4a205feb44403abe698f4f0e2522f9-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Bears</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">are</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">dreamt</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">your</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">fantasies.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9c4a205feb44403abe698f4f0e2522f9-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9c4a205feb44403abe698f4f0e2522f9-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NER\n",
        "\n",
        "The goal of named-entity recognitionÂ is to identify and categorise named entities found in unstructured text into predefined groups, such as names of people, places, organisations, things, medical codes, amounts, numbers, dollar amounts, percentages, etc.\n",
        "\n",
        "Read more: [link](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)"
      ],
      "metadata": {
        "id": "YEqWYZ4e0MTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"Ellenore Smith started working on self-driving cars at Tesla in 2007.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Uq2Fv5SUyrDu",
        "outputId": "5c410cff-7c02-4d16-8258-c60c18ea2def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ellenore Smith\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " started working on self-driving cars at \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2007\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b_UA1Ryjx4Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAupmkbY5guC"
      },
      "source": [
        "\n",
        "### Conclusion\n",
        "In this era of history, we see computers and machines help us in every aspect of our lives! In return we have to help them understand our language better and make the interaction easier for both us humans and machines. \n",
        "\n",
        "Cleaning is just one of the ways that bring about faster and more accurate models. But because itâ€™s modifying the main text, we have to be careful to construct functions that remove as little as possible from the text and its essential parts.\n",
        "\n",
        "Thanks for reading this article!\n",
        "\n",
        "### References\n",
        "\n",
        "[1] https://www.kaggle.com/code/mitramir5/simple-bert-with-video\n",
        "\n",
        "[2] https://bloggingx.com/stop-words/#:~:text=Search%20engines%2C%20in%20both%20search,are%20ignored%20or%20filtered%20out.\n",
        "\n",
        "Image credit: https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/\n",
        "\n",
        "https://www.kaggle.com/code/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n",
        "\n",
        "https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n",
        "\n",
        "https://regexr.com/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "adfccd5c958d70cd78f22c942a5f90dd259f52bf203482d766a0ca6ec5e2e798"
      }
    },
    "colab": {
      "name": "1-data cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}